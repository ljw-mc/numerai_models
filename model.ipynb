{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Installs & Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet numerapi \n",
    "# !pip install --quiet pyarrow \n",
    "# !pip install --quiet fastparquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Checks for the availability of GPUs \n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print('GPU is available.')\n",
    "else:\n",
    "    print('GPU is not available.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numerapi\n",
    "napi = numerapi.NumerAPI('GVTNYX7YW4BUPFPC52ZUM3C5IDT2CUU4', 'EKGP4IDIK6U2XEM3ZVY57PZXAVFG2TMMAAAS6EW7EB5EVVZAKTJZ5INGBWRLJCFS')\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"validation.parquet\").iloc[140000:240000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(df[df.columns[2:1588]].values)\n",
    "y_train = np.nan_to_num(df[df.columns[1588]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the original numpy array\n",
    "# print(y_train.shape)\n",
    "# # Define the number of classes\n",
    "# num_classes = 5\n",
    "\n",
    "# # Convert the array to integers\n",
    "# integer_array = (y_train * (num_classes-1)).astype(int)\n",
    "\n",
    "# # Convert the integer array to one-hot encoded form\n",
    "# one_hot_array = tf.one_hot(integer_array, depth=num_classes)\n",
    "# print(one_hot_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Find unique elements\n",
    "# unique_elements = np.unique(X_train)\n",
    "\n",
    "# # Convert numpy array to list\n",
    "# unique_list = unique_elements.tolist()\n",
    "\n",
    "# print(unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(np_feature_array, np_target_array, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(type(X_train))\n",
    "# X_test = scaler.fit_transform(_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Define the learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.001\n",
    "    drop_rate = 0.5\n",
    "    epochs_drop = 20.0\n",
    "    lr = initial_lr * np.power(drop_rate, np.floor(epoch/epochs_drop))\n",
    "    return lr\n",
    "\n",
    "# Instantiate the learning rate scheduler\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Define the optimizer with the initial learning rate\n",
    "optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 15:03:17.501687: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.2027 - val_loss: 0.0917 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.0936 - val_loss: 0.0908 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.0948 - val_loss: 0.0916 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.0932 - val_loss: 0.0978 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.0940 - val_loss: 0.0861 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.0945 - val_loss: 0.1035 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.0937 - val_loss: 0.0895 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 15s 6ms/step - loss: 0.0944 - val_loss: 0.1022 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, regularizers, callbacks\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "reg_factor = 0.01\n",
    "\n",
    "LRmodel = tf.keras.Sequential([layers.Dense(\n",
    "        1,\n",
    "        input_dim=num_features, \n",
    "        kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "        activation='linear')\n",
    "])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value\n",
    ")\n",
    "\n",
    "\n",
    "# LRmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "LRmodel.compile(loss=tf.keras.losses.MSE, optimizer=optimizer)\n",
    "\n",
    "# Train the model\n",
    "history = LRmodel.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, callbacks=[early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "del X_train\n",
    "del y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(\"train.parquet\").iloc[100000:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.nan_to_num(df[df.columns[2:1588]].values)\n",
    "# y_train = np.nan_to_num(df[df.columns[1588]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df\n",
    "# gc.collect()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(type(X_train))\n",
    "# # X_test = scaler.fit_transform(_test)\n",
    "# history = LRmodel.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, callbacks=[early_stopping, lr_scheduler])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def numerai_corr(preds, target):\n",
    "  # rank (keeping ties) then Gaussianize predictions to standardize prediction distributions\n",
    "  ranked_preds = (preds.rank(method=\"average\").values - 0.5) / preds.count()\n",
    "  gauss_ranked_preds = stats.norm.ppf(ranked_preds)\n",
    "  \n",
    "  # make targets centered around 0\n",
    "  centered_target = target - target.mean()\n",
    "  \n",
    "  # raise both preds and target to the power of 1.5 to accentuate the tails\n",
    "  preds_p15 = np.sign(gauss_ranked_preds) * np.abs(gauss_ranked_preds) ** 1.5\n",
    "  target_p15 = np.sign(centered_target) * np.abs(centered_target) ** 1.5\n",
    "  \n",
    "  # finally return the Pearson correlation\n",
    "  return np.corrcoef(preds_p15, target_p15)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val = pd.read_parquet(\"validation.parquet\").iloc[:70000]\n",
    "df_val = pd.read_parquet(\"train.parquet\").iloc[140000:240000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = scaler.fit_transform(np.nan_to_num(df_val[df_val.columns[2:1588]].values))\n",
    "y_test = np.nan_to_num(df_val[df_val.columns[1588]].values)\n",
    "\n",
    "del df_val\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 4s 1ms/step\n",
      "[[0.5 ]\n",
      " [0.5 ]\n",
      " [0.5 ]\n",
      " ...\n",
      " [0.5 ]\n",
      " [0.5 ]\n",
      " [0.75]]\n",
      "(100000, 1) (100000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Assume `X_test` is your test features tensor and `y_test` is your test targets tensor\n",
    "\n",
    "# Use the trained model to predict on the test set\n",
    "y_pred = LRmodel.predict(X_test)\n",
    "type(y_pred)\n",
    "rounded_pred = np.round(y_pred * 4) / 4\n",
    "print(rounded_pred)\n",
    "print(y_pred.shape, rounded_pred.shape)\n",
    "# print(y_pred[2], y_test[2])\n",
    "\n",
    "# # Calculate the Mean Squared Error (MSE) between the predicted and actual results\n",
    "# mse = tf.keras.losses.mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# # If mse is a tensor, you can convert it to a numpy array for further calculations or analysis\n",
    "# # mse = mse.numpy()\n",
    "\n",
    "# print(\"Test MSE: \", mse)\n",
    "# # print(f\"MSE shape:{mse.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our predictions:     [0.46 0.34 0.43 0.57 0.57 0.77 0.44 0.38 0.36 0.54 0.6  0.48 0.4  0.57]\n",
      "rounded predictions: [0.5  0.25 0.5  0.5  0.5  0.75 0.5  0.5  0.25 0.5  0.5  0.5 ]\n",
      "actual results:      [0.   0.75 0.25 0.5  0.5  0.5  0.75 0.75 0.5  0.5  0.25 0.25 0.5  0.5 ]\n"
     ]
    }
   ],
   "source": [
    "y_pred = y_pred.flatten()\n",
    "rounded_pred = rounded_pred.flatten()\n",
    "k = 200\n",
    "print(f\"our predictions:     {np.round(y_pred[k:20-6+k], decimals=2)}\")\n",
    "print(f\"rounded predictions: {np.round(rounded_pred[k:20-8+k], decimals=2)}\")\n",
    "print(f\"actual results:      {y_test[k:20-6+k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01078185349394332\n",
      "0.01025874699381771\n"
     ]
    }
   ],
   "source": [
    "print(numerai_corr(preds=pd.Series(y_pred), target=y_test))\n",
    "print(numerai_corr(preds=pd.Series(rounded_pred), target=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.19201629, shape=(), dtype=float32)\n",
      "tf.Tensor(0.1796525, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "total_mse = tf.reduce_mean(tf.keras.losses.MAE(y_test, y_pred))\n",
    "print(total_mse)\n",
    "print(tf.reduce_mean(tf.keras.losses.MAE(y_test, rounded_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = np.array([numerai_corr(preds=pd.Series(np.random.uniform(0, 1, 100000)), target=y_test) for _ in range(2000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: -6.674273863670657e-05\n",
      "std dev: 0.003168503888445623\n"
     ]
    }
   ],
   "source": [
    "print(\"mean:\", result_array.mean())\n",
    "print(\"std dev:\", np.std(result_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now, generate the numpy array\n",
    "# result_array = np.array([numerai_corr(preds=pd.Series(np.random.uniform(0, 1, 70000)), target=y_test) for _ in range(2000)])\n",
    "# print(\"mean:\", result_array.mean())\n",
    "# print(\"std dev:\", np.std(result_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_test\n",
    "# del y_test\n",
    "# gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "154/154 [==============================] - 0s 1ms/step\n",
      "[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      "[0.25, 0.5, 0.75]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get current round\n",
    "current_round = napi.get_current_round()\n",
    "print(current_round)\n",
    "\n",
    "# # Download latest live features\n",
    "# napi.download_dataset(f\"v4.1/live_{current_round}.parquet\")\n",
    "live_data = pd.read_parquet(f\"499_v4_1_live.parquet\")\n",
    "live_features = live_data[[f for f in live_data.columns if \"feature\" in f]]\n",
    "\n",
    "# # Generate live predictions\n",
    "live_predictions = LRmodel.predict(live_features).flatten()\n",
    "live_predictions = np.round(live_predictions * 4) / 4\n",
    "print(live_predictions)\n",
    "\n",
    "# Find unique elements\n",
    "unique_elements = np.unique(live_predictions)\n",
    "\n",
    "# Convert numpy array to list\n",
    "unique_list = unique_elements.tolist()\n",
    "\n",
    "print(unique_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format submission\n",
    "submission = pd.Series(live_predictions, index=live_features.index).to_frame(\"prediction\")\n",
    "submission.to_csv(f\"prediction_{current_round}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 15:07:22,596 INFO numerapi.base_api: uploading predictions...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e917952a-4c03-4414-8949-8f06103853f6'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload submission \n",
    "napi.upload_predictions(f\"prediction_{current_round}.csv\", model_id=\"6e2dad38-a2db-497f-8061-d296f2a762eb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
